{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264347ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-16 12:04:55.614506: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-07-16 12:04:55.688630: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-16 12:04:55.688658: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import add, concatenate\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import plot_model\n",
    "import tensorflow\n",
    "from tensorflow.keras.regularizers import L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53aed78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-16 12:04:59.346072: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-16 12:04:59.346138: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-16 12:04:59.346187: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pthpth): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500c9f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4905f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch operation usng tensor slice\n",
    "def WaveletTransformAxisY(batch_img):\n",
    "    odd_img  = batch_img[:,0::2]\n",
    "    even_img = batch_img[:,1::2]\n",
    "    L = (odd_img + even_img) / 2.0\n",
    "    H = K.abs(odd_img - even_img)\n",
    "    return L, H\n",
    "\n",
    "def WaveletTransformAxisX(batch_img):\n",
    "    # transpose + fliplr\n",
    "    tmp_batch = K.permute_dimensions(batch_img, [0, 2, 1])[:,:,::-1]\n",
    "    _dst_L, _dst_H = WaveletTransformAxisY(tmp_batch)\n",
    "    # transpose + flipud\n",
    "    dst_L = K.permute_dimensions(_dst_L, [0, 2, 1])[:,::-1,...]\n",
    "    dst_H = K.permute_dimensions(_dst_H, [0, 2, 1])[:,::-1,...]\n",
    "    return dst_L, dst_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0fcc6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wavelet(batch_image):\n",
    "    # make channel first image\n",
    "    batch_image = K.permute_dimensions(batch_image, [0, 3, 1, 2])\n",
    "    r = batch_image[:,0]\n",
    "    g = batch_image[:,1]\n",
    "    b = batch_image[:,2]\n",
    "\n",
    "    # level 1 decomposition\n",
    "    wavelet_L, wavelet_H = WaveletTransformAxisY(r)\n",
    "    r_wavelet_LL, r_wavelet_LH = WaveletTransformAxisX(wavelet_L)\n",
    "    r_wavelet_HL, r_wavelet_HH = WaveletTransformAxisX(wavelet_H)\n",
    "\n",
    "    wavelet_L, wavelet_H = WaveletTransformAxisY(g)\n",
    "    g_wavelet_LL, g_wavelet_LH = WaveletTransformAxisX(wavelet_L)\n",
    "    g_wavelet_HL, g_wavelet_HH = WaveletTransformAxisX(wavelet_H)\n",
    "\n",
    "    wavelet_L, wavelet_H = WaveletTransformAxisY(b)\n",
    "    b_wavelet_LL, b_wavelet_LH = WaveletTransformAxisX(wavelet_L)\n",
    "    b_wavelet_HL, b_wavelet_HH = WaveletTransformAxisX(wavelet_H)\n",
    "\n",
    "    wavelet_data = [r_wavelet_LL, r_wavelet_LH, r_wavelet_HL, r_wavelet_HH, \n",
    "                    g_wavelet_LL, g_wavelet_LH, g_wavelet_HL, g_wavelet_HH,\n",
    "                    b_wavelet_LL, b_wavelet_LH, b_wavelet_HL, b_wavelet_HH]\n",
    "    transform_batch = K.stack(wavelet_data, axis=1)\n",
    "\n",
    "    # level 2 decomposition\n",
    "    wavelet_L2, wavelet_H2 = WaveletTransformAxisY(r_wavelet_LL)\n",
    "    r_wavelet_LL2, r_wavelet_LH2 = WaveletTransformAxisX(wavelet_L2)\n",
    "    r_wavelet_HL2, r_wavelet_HH2 = WaveletTransformAxisX(wavelet_H2)\n",
    "\n",
    "    wavelet_L2, wavelet_H2 = WaveletTransformAxisY(g_wavelet_LL)\n",
    "    g_wavelet_LL2, g_wavelet_LH2 = WaveletTransformAxisX(wavelet_L2)\n",
    "    g_wavelet_HL2, g_wavelet_HH2 = WaveletTransformAxisX(wavelet_H2)\n",
    "\n",
    "    wavelet_L2, wavelet_H2 = WaveletTransformAxisY(b_wavelet_LL)\n",
    "    b_wavelet_LL2, b_wavelet_LH2 = WaveletTransformAxisX(wavelet_L2)\n",
    "    b_wavelet_HL2, b_wavelet_HH2 = WaveletTransformAxisX(wavelet_H2)\n",
    "\n",
    "\n",
    "    wavelet_data_l2 = [r_wavelet_LL2, r_wavelet_LH2, r_wavelet_HL2, r_wavelet_HH2, \n",
    "                    g_wavelet_LL2, g_wavelet_LH2, g_wavelet_HL2, g_wavelet_HH2,\n",
    "                    b_wavelet_LL2, b_wavelet_LH2, b_wavelet_HL2, b_wavelet_HH2]\n",
    "    transform_batch_l2 = K.stack(wavelet_data_l2, axis=1)\n",
    "\n",
    "    # level 3 decomposition\n",
    "    wavelet_L3, wavelet_H3 = WaveletTransformAxisY(r_wavelet_LL2)\n",
    "    r_wavelet_LL3, r_wavelet_LH3 = WaveletTransformAxisX(wavelet_L3)\n",
    "    r_wavelet_HL3, r_wavelet_HH3 = WaveletTransformAxisX(wavelet_H3)\n",
    "\n",
    "    wavelet_L3, wavelet_H3 = WaveletTransformAxisY(g_wavelet_LL2)\n",
    "    g_wavelet_LL3, g_wavelet_LH3 = WaveletTransformAxisX(wavelet_L3)\n",
    "    g_wavelet_HL3, g_wavelet_HH3 = WaveletTransformAxisX(wavelet_H3)\n",
    "\n",
    "    wavelet_L3, wavelet_H3 = WaveletTransformAxisY(b_wavelet_LL2)\n",
    "    b_wavelet_LL3, b_wavelet_LH3 = WaveletTransformAxisX(wavelet_L3)\n",
    "    b_wavelet_HL3, b_wavelet_HH3 = WaveletTransformAxisX(wavelet_H3)\n",
    "\n",
    "    wavelet_data_l3 = [r_wavelet_LL3, r_wavelet_LH3, r_wavelet_HL3, r_wavelet_HH3, \n",
    "                    g_wavelet_LL3, g_wavelet_LH3, g_wavelet_HL3, g_wavelet_HH3,\n",
    "                    b_wavelet_LL3, b_wavelet_LH3, b_wavelet_HL3, b_wavelet_HH3]\n",
    "    transform_batch_l3 = K.stack(wavelet_data_l3, axis=1)\n",
    "\n",
    "    # level 4 decomposition\n",
    "    wavelet_L4, wavelet_H4 = WaveletTransformAxisY(r_wavelet_LL3)\n",
    "    r_wavelet_LL4, r_wavelet_LH4 = WaveletTransformAxisX(wavelet_L4)\n",
    "    r_wavelet_HL4, r_wavelet_HH4 = WaveletTransformAxisX(wavelet_H4)\n",
    "\n",
    "    wavelet_L4, wavelet_H4 = WaveletTransformAxisY(g_wavelet_LL3)\n",
    "    g_wavelet_LL4, g_wavelet_LH4 = WaveletTransformAxisX(wavelet_L4)\n",
    "    g_wavelet_HL4, g_wavelet_HH4 = WaveletTransformAxisX(wavelet_H4)\n",
    "\n",
    "    wavelet_L3, wavelet_H3 = WaveletTransformAxisY(b_wavelet_LL3)\n",
    "    b_wavelet_LL4, b_wavelet_LH4 = WaveletTransformAxisX(wavelet_L4)\n",
    "    b_wavelet_HL4, b_wavelet_HH4 = WaveletTransformAxisX(wavelet_H4)\n",
    "\n",
    "\n",
    "    wavelet_data_l4 = [r_wavelet_LL4, r_wavelet_LH4, r_wavelet_HL4, r_wavelet_HH4, \n",
    "                    g_wavelet_LL4, g_wavelet_LH4, g_wavelet_HL4, g_wavelet_HH4,\n",
    "                    b_wavelet_LL4, b_wavelet_LH4, b_wavelet_HL4, b_wavelet_HH4]\n",
    "    transform_batch_l4 = K.stack(wavelet_data_l4, axis=1)\n",
    "\n",
    "    # print('shape before')\n",
    "    # print(transform_batch.shape)\n",
    "    # print(transform_batch_l2.shape)\n",
    "    # print(transform_batch_l3.shape)\n",
    "    # print(transform_batch_l4.shape)\n",
    "\n",
    "    decom_level_1 = K.permute_dimensions(transform_batch, [0, 2, 3, 1])\n",
    "    decom_level_2 = K.permute_dimensions(transform_batch_l2, [0, 2, 3, 1])\n",
    "    decom_level_3 = K.permute_dimensions(transform_batch_l3, [0, 2, 3, 1])\n",
    "    decom_level_4 = K.permute_dimensions(transform_batch_l4, [0, 2, 3, 1])\n",
    "    \n",
    "    # print('shape after')\n",
    "    # print(decom_level_1.shape)\n",
    "    # print(decom_level_2.shape)\n",
    "    # print(decom_level_3.shape)\n",
    "    # print(decom_level_4.shape)\n",
    "    return [decom_level_1, decom_level_2, decom_level_3, decom_level_4]\n",
    "\n",
    "\n",
    "def Wavelet_out_shape(input_shapes):\n",
    "    # print('in to shape')\n",
    "    return [tuple([None, 96, 96, 12]), tuple([None, 48, 48, 12]), \n",
    "            tuple([None, 24, 24, 12]), tuple([None, 12, 12, 12])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc9955a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-16 12:05:23.921452: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TensorShape([8, 112, 112, 12]),\n",
       " TensorShape([8, 56, 56, 12]),\n",
       " TensorShape([8, 28, 28, 12]),\n",
       " TensorShape([8, 14, 14, 12])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_batch = K.zeros(shape=(8, 224, 224, 3), dtype='float32')\n",
    "[x.shape for x in Wavelet(img_batch)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7d39741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wavelet_cnn_model():\n",
    "\n",
    "    input_shape = 192, 192, 3\n",
    "\n",
    "    input_ = Input(input_shape, name='the_input')\n",
    "    # wavelet = Lambda(Wavelet, name='wavelet')\n",
    "    wavelet = Lambda(Wavelet, Wavelet_out_shape, name='wavelet')\n",
    "    input_l1, input_l2, input_l3, input_l4 = wavelet(input_)\n",
    "    # print(input_l1)\n",
    "    # print(input_l2)\n",
    "    # print(input_l3)\n",
    "    # print(input_l4)\n",
    "    # level one decomposition starts\n",
    "    conv_1 = Conv2D(64, kernel_size=(3, 3), padding='same', name='conv_1')(input_l1)\n",
    "    norm_1 = BatchNormalization(name='norm_1')(conv_1)\n",
    "    relu_1 = Activation('relu', name='relu_1')(norm_1)\n",
    "\n",
    "    conv_1_2 = Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_1_2')(relu_1)\n",
    "    norm_1_2 = BatchNormalization(name='norm_1_2')(conv_1_2)\n",
    "    relu_1_2 = Activation('relu', name='relu_1_2')(norm_1_2)\n",
    "\n",
    "    # level two decomposition starts\n",
    "    conv_a = Conv2D(filters=64, kernel_size=(3, 3), padding='same', name='conv_a')(input_l2)\n",
    "    norm_a = BatchNormalization(name='norm_a')(conv_a)\n",
    "    relu_a = Activation('relu', name='relu_a')(norm_a)\n",
    "\n",
    "    # concate level one and level two decomposition\n",
    "    concate_level_2 = concatenate([relu_1_2, relu_a])\n",
    "    conv_2 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_2')(concate_level_2)\n",
    "    norm_2 = BatchNormalization(name='norm_2')(conv_2)\n",
    "    relu_2 = Activation('relu', name='relu_2')(norm_2)\n",
    "\n",
    "    conv_2_2 = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_2_2')(relu_2)\n",
    "    norm_2_2 = BatchNormalization(name='norm_2_2')(conv_2_2)\n",
    "    relu_2_2 = Activation('relu', name='relu_2_2')(norm_2_2)\n",
    "\n",
    "    # level three decomposition starts \n",
    "    conv_b = Conv2D(filters=64, kernel_size=(3, 3), padding='same', name='conv_b')(input_l3)\n",
    "    norm_b = BatchNormalization(name='norm_b')(conv_b)\n",
    "    relu_b = Activation('relu', name='relu_b')(norm_b)\n",
    "\n",
    "    conv_b_2 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_b_2')(relu_b)\n",
    "    norm_b_2 = BatchNormalization(name='norm_b_2')(conv_b_2)\n",
    "    relu_b_2 = Activation('relu', name='relu_b_2')(norm_b_2)\n",
    "\n",
    "    # concate level two and level three decomposition \n",
    "    concate_level_3 = concatenate([relu_2_2, relu_b_2])\n",
    "    conv_3 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_3')(concate_level_3)\n",
    "    norm_3 = BatchNormalization(name='nomr_3')(conv_3)\n",
    "    relu_3 = Activation('relu', name='relu_3')(norm_3)\n",
    "\n",
    "    conv_3_2 = Conv2D(256, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_3_2')(relu_3)\n",
    "    norm_3_2 = BatchNormalization(name='norm_3_2')(conv_3_2)\n",
    "    relu_3_2 = Activation('relu', name='relu_3_2')(norm_3_2)\n",
    "\n",
    "    # level four decomposition start\n",
    "    conv_c = Conv2D(64, kernel_size=(3, 3), padding='same', name='conv_c')(input_l4)\n",
    "    norm_c = BatchNormalization(name='norm_c')(conv_c)\n",
    "    relu_c = Activation('relu', name='relu_c')(norm_c)\n",
    "\n",
    "    conv_c_2 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_c_2')(relu_c)\n",
    "    norm_c_2 = BatchNormalization(name='norm_c_2')(conv_c_2)\n",
    "    relu_c_2 = Activation('relu', name='relu_c_2')(norm_c_2)\n",
    "\n",
    "    conv_c_3 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_c_3')(relu_c_2)\n",
    "    norm_c_3 = BatchNormalization(name='norm_c_3')(conv_c_3)\n",
    "    relu_c_3 = Activation('relu', name='relu_c_3')(norm_c_3)\n",
    "\n",
    "    # concate level level three and level four decomposition\n",
    "    concate_level_4 = concatenate([relu_3_2, relu_c_3])\n",
    "    conv_4 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_4')(concate_level_4)\n",
    "    norm_4 = BatchNormalization(name='norm_4')(conv_4)\n",
    "    relu_4 = Activation('relu', name='relu_4')(norm_4)\n",
    "\n",
    "    conv_4_2 = Conv2D(256, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_4_2')(relu_4)\n",
    "    norm_4_2 = BatchNormalization(name='norm_4_2')(conv_4_2)\n",
    "    relu_4_2 = Activation('relu', name='relu_4_2')(norm_4_2)\n",
    "\n",
    "    conv_5_1 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_5_1')(relu_4_2)\n",
    "    norm_5_1 = BatchNormalization(name='norm_5_1')(conv_5_1)\n",
    "    relu_5_1 = Activation('relu', name='relu_5_1')(norm_5_1)\n",
    "\n",
    "    pool_5_1 = AveragePooling2D(pool_size=(7, 7), strides=1, padding='same', name='avg_pool_5_1')(relu_5_1)\n",
    "    flat_5_1 = Flatten(name='flat_5_1')(pool_5_1) \n",
    "\n",
    "    fc_5 = Dense(2048, name='fc_5')(flat_5_1)\n",
    "    norm_5 = BatchNormalization(name='norm_5')(fc_5)\n",
    "    relu_5 = Activation('relu', name='relu_5')(norm_5)\n",
    "    drop_5 = Dropout(0.5, name='drop_5')(relu_5)\n",
    "\n",
    "    fc_6 = Dense(2048, name='fc_6')(drop_5)\n",
    "    norm_6 = BatchNormalization(name='norm_6')(fc_6)\n",
    "    relu_6 = Activation('relu', name='relu_6')(norm_6)\n",
    "    drop_6 = Dropout(0.5, name='drop_6')(relu_6)\n",
    "\n",
    "    output = Dense(47, activation='softmax', name='fc_7')(drop_6)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=output)\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='wavelet_cnn_0.5.png')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8c4d0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " the_input (InputLayer)         [(None, 192, 192, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " wavelet (Lambda)               [(None, 96, 96, 12)  0           ['the_input[0][0]']              \n",
      "                                , (None, 48, 48, 12                                               \n",
      "                                ),                                                                \n",
      "                                 (None, 24, 24, 12)                                               \n",
      "                                , (None, 12, 12, 12                                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv_1 (Conv2D)                (None, 96, 96, 64)   6976        ['wavelet[0][0]']                \n",
      "                                                                                                  \n",
      " norm_1 (BatchNormalization)    (None, 96, 96, 64)   256         ['conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " relu_1 (Activation)            (None, 96, 96, 64)   0           ['norm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv_1_2 (Conv2D)              (None, 48, 48, 64)   36928       ['relu_1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv_a (Conv2D)                (None, 48, 48, 64)   6976        ['wavelet[0][1]']                \n",
      "                                                                                                  \n",
      " norm_1_2 (BatchNormalization)  (None, 48, 48, 64)   256         ['conv_1_2[0][0]']               \n",
      "                                                                                                  \n",
      " norm_a (BatchNormalization)    (None, 48, 48, 64)   256         ['conv_a[0][0]']                 \n",
      "                                                                                                  \n",
      " relu_1_2 (Activation)          (None, 48, 48, 64)   0           ['norm_1_2[0][0]']               \n",
      "                                                                                                  \n",
      " relu_a (Activation)            (None, 48, 48, 64)   0           ['norm_a[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 48, 48, 128)  0           ['relu_1_2[0][0]',               \n",
      "                                                                  'relu_a[0][0]']                 \n",
      "                                                                                                  \n",
      " conv_2 (Conv2D)                (None, 48, 48, 128)  147584      ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv_b (Conv2D)                (None, 24, 24, 64)   6976        ['wavelet[0][2]']                \n",
      "                                                                                                  \n",
      " norm_2 (BatchNormalization)    (None, 48, 48, 128)  512         ['conv_2[0][0]']                 \n",
      "                                                                                                  \n",
      " norm_b (BatchNormalization)    (None, 24, 24, 64)   256         ['conv_b[0][0]']                 \n",
      "                                                                                                  \n",
      " relu_2 (Activation)            (None, 48, 48, 128)  0           ['norm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " relu_b (Activation)            (None, 24, 24, 64)   0           ['norm_b[0][0]']                 \n",
      "                                                                                                  \n",
      " conv_2_2 (Conv2D)              (None, 24, 24, 128)  147584      ['relu_2[0][0]']                 \n",
      "                                                                                                  \n",
      " conv_b_2 (Conv2D)              (None, 24, 24, 128)  73856       ['relu_b[0][0]']                 \n",
      "                                                                                                  \n",
      " norm_2_2 (BatchNormalization)  (None, 24, 24, 128)  512         ['conv_2_2[0][0]']               \n",
      "                                                                                                  \n",
      " norm_b_2 (BatchNormalization)  (None, 24, 24, 128)  512         ['conv_b_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv_c (Conv2D)                (None, 12, 12, 64)   6976        ['wavelet[0][3]']                \n",
      "                                                                                                  \n",
      " relu_2_2 (Activation)          (None, 24, 24, 128)  0           ['norm_2_2[0][0]']               \n",
      "                                                                                                  \n",
      " relu_b_2 (Activation)          (None, 24, 24, 128)  0           ['norm_b_2[0][0]']               \n",
      "                                                                                                  \n",
      " norm_c (BatchNormalization)    (None, 12, 12, 64)   256         ['conv_c[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 24, 24, 256)  0           ['relu_2_2[0][0]',               \n",
      "                                                                  'relu_b_2[0][0]']               \n",
      "                                                                                                  \n",
      " relu_c (Activation)            (None, 12, 12, 64)   0           ['norm_c[0][0]']                 \n",
      "                                                                                                  \n",
      " conv_3 (Conv2D)                (None, 24, 24, 256)  590080      ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv_c_2 (Conv2D)              (None, 12, 12, 256)  147712      ['relu_c[0][0]']                 \n",
      "                                                                                                  \n",
      " nomr_3 (BatchNormalization)    (None, 24, 24, 256)  1024        ['conv_3[0][0]']                 \n",
      "                                                                                                  \n",
      " norm_c_2 (BatchNormalization)  (None, 12, 12, 256)  1024        ['conv_c_2[0][0]']               \n",
      "                                                                                                  \n",
      " relu_3 (Activation)            (None, 24, 24, 256)  0           ['nomr_3[0][0]']                 \n",
      "                                                                                                  \n",
      " relu_c_2 (Activation)          (None, 12, 12, 256)  0           ['norm_c_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv_3_2 (Conv2D)              (None, 12, 12, 256)  590080      ['relu_3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv_c_3 (Conv2D)              (None, 12, 12, 256)  590080      ['relu_c_2[0][0]']               \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " norm_3_2 (BatchNormalization)  (None, 12, 12, 256)  1024        ['conv_3_2[0][0]']               \n",
      "                                                                                                  \n",
      " norm_c_3 (BatchNormalization)  (None, 12, 12, 256)  1024        ['conv_c_3[0][0]']               \n",
      "                                                                                                  \n",
      " relu_3_2 (Activation)          (None, 12, 12, 256)  0           ['norm_3_2[0][0]']               \n",
      "                                                                                                  \n",
      " relu_c_3 (Activation)          (None, 12, 12, 256)  0           ['norm_c_3[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 12, 12, 512)  0           ['relu_3_2[0][0]',               \n",
      "                                                                  'relu_c_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv_4 (Conv2D)                (None, 12, 12, 256)  1179904     ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " norm_4 (BatchNormalization)    (None, 12, 12, 256)  1024        ['conv_4[0][0]']                 \n",
      "                                                                                                  \n",
      " relu_4 (Activation)            (None, 12, 12, 256)  0           ['norm_4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv_4_2 (Conv2D)              (None, 6, 6, 256)    590080      ['relu_4[0][0]']                 \n",
      "                                                                                                  \n",
      " norm_4_2 (BatchNormalization)  (None, 6, 6, 256)    1024        ['conv_4_2[0][0]']               \n",
      "                                                                                                  \n",
      " relu_4_2 (Activation)          (None, 6, 6, 256)    0           ['norm_4_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv_5_1 (Conv2D)              (None, 6, 6, 128)    295040      ['relu_4_2[0][0]']               \n",
      "                                                                                                  \n",
      " norm_5_1 (BatchNormalization)  (None, 6, 6, 128)    512         ['conv_5_1[0][0]']               \n",
      "                                                                                                  \n",
      " relu_5_1 (Activation)          (None, 6, 6, 128)    0           ['norm_5_1[0][0]']               \n",
      "                                                                                                  \n",
      " avg_pool_5_1 (AveragePooling2D  (None, 6, 6, 128)   0           ['relu_5_1[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " flat_5_1 (Flatten)             (None, 4608)         0           ['avg_pool_5_1[0][0]']           \n",
      "                                                                                                  \n",
      " fc_5 (Dense)                   (None, 2048)         9439232     ['flat_5_1[0][0]']               \n",
      "                                                                                                  \n",
      " norm_5 (BatchNormalization)    (None, 2048)         8192        ['fc_5[0][0]']                   \n",
      "                                                                                                  \n",
      " relu_5 (Activation)            (None, 2048)         0           ['norm_5[0][0]']                 \n",
      "                                                                                                  \n",
      " drop_5 (Dropout)               (None, 2048)         0           ['relu_5[0][0]']                 \n",
      "                                                                                                  \n",
      " fc_6 (Dense)                   (None, 2048)         4196352     ['drop_5[0][0]']                 \n",
      "                                                                                                  \n",
      " norm_6 (BatchNormalization)    (None, 2048)         8192        ['fc_6[0][0]']                   \n",
      "                                                                                                  \n",
      " relu_6 (Activation)            (None, 2048)         0           ['norm_6[0][0]']                 \n",
      "                                                                                                  \n",
      " drop_6 (Dropout)               (None, 2048)         0           ['relu_6[0][0]']                 \n",
      "                                                                                                  \n",
      " fc_7 (Dense)                   (None, 47)           96303       ['drop_6[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 18,174,575\n",
      "Trainable params: 18,161,647\n",
      "Non-trainable params: 12,928\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_wavelet_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abcbf867",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=tensorflow.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "243494ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'dtd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e000d87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5076 images belonging to 47 classes.\n",
      "Found 564 images belonging to 47 classes.\n",
      "Epoch 1/40\n",
      " 88/318 [=======>......................] - ETA: 1:55 - loss: 8.3002 - accuracy: 0.0241"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m      8\u001b[0m     train_data_dir,\n\u001b[1;32m      9\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m192\u001b[39m, \u001b[38;5;241m192\u001b[39m),\n\u001b[1;32m     10\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     11\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# set as training data\u001b[39;00m\n\u001b[1;32m     14\u001b[0m validation_generator \u001b[38;5;241m=\u001b[39m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m     15\u001b[0m     train_data_dir, \u001b[38;5;66;03m# same directory as training data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m192\u001b[39m, \u001b[38;5;241m192\u001b[39m),\n\u001b[1;32m     17\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     18\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# set as validation a\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m history\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.1) # et validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(192, 192),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir, # same directory as training data\n",
    "    target_size=(192, 192),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation a\n",
    "\n",
    "history=model.fit(\n",
    "    train_generator,\n",
    "    validation_data = validation_generator, \n",
    "    epochs = 40,\n",
    "    callbacks=[tensorflow.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dac69d61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      3\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "print(history.history.keys())\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(acc, label='training accuracy')\n",
    "plt.plot(val_acc, label='validation accuracy')\n",
    "plt.title('Accuracy curve')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0164944b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(loss, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(loss, label='training loss')\n",
    "plt.plot(val_loss, label='validation loss')\n",
    "plt.title('Loss curve')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f4732",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_model\\WaveLetCNN-dtd')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
